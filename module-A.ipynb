{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Group 1\n",
    "\n",
    "## Module A _(25 points)_\n",
    "\n",
    "__A1.__ _(3 points)_ In this module, you will be working with the [Seinfeld Chronicles dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles). Create an account on [Kaggle](https://www.kaggle.com) and download the `scripts.csv` file from the dataset and move it into the `data` directory. Read the `data/scripts.csv` file as a text file line-by-line and examine the list you have loaded the data into. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#readFile = open('data/scripts.csv', 'r')\n",
    "#for line in readFile:\n",
    "#    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2.__ _(2 points)_ Is it possible to work with this data, simply splitting by a delimiter? Explain any complexity in the data's structured format that necessitates an established format-specific file reader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to read through the script and get an idea of what is being said. but for quatitative purposes, it is not possible to work with this data. There are so many delimiters that it would be very difficult to compare types of data in each line; additionally, commas used as punctiontion in the lines would also be reccognized as delimiters, causing splits where we don't want them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3.__ _(5 points)_ Use the `csv` module to read the contents of the `data/scripts.csv` file into a list. Examine this list. How many unique speaking characters are present in the scripts in total? \n",
    "\n",
    "__Important__: please don't get stuck on cleaning text in this module! It's great to take note of issues in data and even address them, but the regular expressions (regex) required to get heavily into that work is ahead in __Chapter 4__ and so not required here. Please just count characters and words as best possible using the topics from Chapters 0&ndash;2 (na√Øvely, even), and utilize the markdown response boxes to discuss what you see as being the challenges in working with these data and what solutions might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 1639 characters in this script.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('data/scripts.csv', 'r') as newfile:\n",
    "    reader = csv.reader(newfile)\n",
    "    selection = list(reader)\n",
    "    # We need to get rid of the header\n",
    "    selection.pop(0)\n",
    "\n",
    "for row in selection:\n",
    "    characters.append(row[1])   \n",
    "\n",
    "character_count = len(set(characters))\n",
    "    \n",
    "print('There are a total of ' + str(character_count) + ' characters in this script.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A4.__ _(2 points)_ Count the dialogue entries for the four major characters, \"JERRY\", \"GEORGE\", \"ELAINE\", and \"KRAMER\", using a dictionary (you are not allowed to use the Counter data structure for any component of this module).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JERRY', 'ELAINE', 'GEORGE', 'KRAMER']\n",
      "14981 8099 9805 6738\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('data/scripts.csv', 'r') as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    reader_list = list(reader)\n",
    "    reader_list.pop(0)\n",
    "\n",
    "#These are the index value counters for the dicitonaries\n",
    "j=0\n",
    "e=0\n",
    "g=0\n",
    "k=0\n",
    "\n",
    "jlines = []\n",
    "elines = []\n",
    "glines = []\n",
    "klines = []\n",
    "\n",
    "for row in reader_list:\n",
    "    if row[1].startswith('JERRY'):\n",
    "        jlines.append(row[2])\n",
    "        j+=1\n",
    "\n",
    "for row in reader_list:\n",
    "    if row[1].startswith('ELAINE'):\n",
    "        elines.append(row[2])\n",
    "        e+=1\n",
    "\n",
    "for row in reader_list:\n",
    "    if row[1].startswith('GEORGE'):\n",
    "        glines.append(row[2])\n",
    "        g+=1\n",
    "\n",
    "for row in reader_list:\n",
    "    if row[1].startswith('KRAMER'):\n",
    "        klines.append(row[2])\n",
    "        k+=1\n",
    "\n",
    "\n",
    "dialogue_dict = {'JERRY': jlines,\n",
    "                'ELAINE': elines,\n",
    "                'GEORGE': glines,\n",
    "                'KRAMER': klines}\n",
    "\n",
    "who_said_what = [name for name in dialogue_dict]\n",
    "print(who_said_what)\n",
    "print(j+1, e+1, g+1, k+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A5.__ _(3 points)_ Count the number of words spoken by each of the main characters using a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jerry said 148986 words.\n",
      "Elaine said 75975 words.\n",
      "George said 107902 words.\n",
      "Kramer said 70931 words.\n"
     ]
    }
   ],
   "source": [
    "jvalues = dialogue_dict[\"JERRY\"]\n",
    "jwords = []\n",
    "for line in jvalues:\n",
    "    jwords+= line.split()\n",
    "\n",
    "evalues = dialogue_dict[\"ELAINE\"]\n",
    "ewords = []\n",
    "for line in evalues:\n",
    "    ewords+= line.split()\n",
    "\n",
    "gvalues = dialogue_dict[\"GEORGE\"]\n",
    "gwords = []\n",
    "for line in gvalues:\n",
    "    gwords+= line.split()\n",
    "\n",
    "kvalues = dialogue_dict[\"KRAMER\"]\n",
    "kwords = []\n",
    "for line in kvalues:\n",
    "    kwords+= line.split()\n",
    "\n",
    "# Must add one because index count starts at 0\n",
    "print(\"Jerry said \" + str(len(jwords)+1) + \" words.\")\n",
    "print(\"Elaine said \" + str(len(ewords)+1) + \" words.\")\n",
    "print(\"George said \" + str(len(gwords)+1) + \" words.\")\n",
    "print(\"Kramer said \" + str(len(kwords)+1) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6.__ _(5 points)_ Count how many times each word is spoken by the main characters using a dictionary, then sort these word counts in descending order, i.e. from the most commonly spoken words to least. [__Hint__: You can use either a lambda function or a list comprehension to do this.] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jerry's top ten:\n",
      "['the', 'I', 'you', 'a', 'to', 'of', 'in', 'and', 'is', 'You']\n",
      "Elaine's top ten:\n",
      "['I', 'the', 'you', 'a', 'to', 'and', 'is', 'of', 'that', 'You']\n",
      "George's top ten:\n",
      "['I', 'the', 'a', 'to', 'you', 'of', 'in', 'and', 'You', \"I'm\"]\n",
      "Kramer's top ten:\n",
      "['the', 'I', 'a', 'you', 'to', 'and', 'of', 'in', \"I'm\", 'You']\n"
     ]
    }
   ],
   "source": [
    "# Each word will be saved to the dictionary as {word:number of occurances}\n",
    "j_word_freq = {}\n",
    "for word in jwords:\n",
    "    if word in j_word_freq:\n",
    "        j_word_freq[word]+=1\n",
    "    else:\n",
    "        j_word_freq[word]=1\n",
    "\n",
    "# Now sort by values\n",
    "j_ordered_word_freq = sorted(j_word_freq, reverse = True, key=lambda x: j_word_freq[x])\n",
    "\n",
    "# And print the top ten words in the frequency sorted list\n",
    "print(\"Jerry's top ten:\")\n",
    "print(j_ordered_word_freq[:10])\n",
    "\n",
    "# Repeat for each main character\n",
    "\n",
    "e_word_freq = {}\n",
    "for word in ewords:\n",
    "    if word in e_word_freq:\n",
    "        e_word_freq[word]+=1\n",
    "    else:\n",
    "        e_word_freq[word]=1\n",
    "e_ordered_word_freq = sorted(e_word_freq, reverse = True, key=lambda x: e_word_freq[x])\n",
    "print(\"Elaine's top ten:\")\n",
    "print(e_ordered_word_freq[:10])\n",
    "\n",
    "g_word_freq = {}\n",
    "for word in gwords:\n",
    "    if word in g_word_freq:\n",
    "        g_word_freq[word]+=1\n",
    "    else:\n",
    "        g_word_freq[word]=1\n",
    "g_ordered_word_freq = sorted(g_word_freq, reverse = True, key=lambda x: g_word_freq[x])\n",
    "print(\"George's top ten:\")\n",
    "print(g_ordered_word_freq[:10])\n",
    "\n",
    "k_word_freq = {}\n",
    "for word in kwords:\n",
    "    if word in k_word_freq:\n",
    "        k_word_freq[word]+=1\n",
    "    else:\n",
    "        k_word_freq[word]=1\n",
    "k_ordered_word_freq = sorted(k_word_freq, reverse = True, key=lambda x: k_word_freq[x])\n",
    "print(\"Kramer's top ten:\")\n",
    "print(k_ordered_word_freq[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A7.__ _(5 points)_ Load the `data/stop-words.txt` file into a list. Find the 10 most common words for each of the main characters that are not in this list of stop words. Put these most common words in a dictionary data strucutre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'JERRY': ['I', 'a', 'of', 'and', 'You', 'it', 'on', \"I'm\", 'have', 'What'], 'ELAINE': ['I', 'you', 'to', 'is', 'that', 'You', 'it', \"I'm\", 'Oh,', 'have'], 'GEORGE': ['I', 'a', 'you', 'in', 'You', \"I'm\", 'it', 'my', 'have', 'for'], 'KRAMER': ['I', 'you', 'and', 'in', \"I'm\", 'You', 'Well,', 'that', 'is', 'Yeah,']}\n"
     ]
    }
   ],
   "source": [
    "# Open stop words\n",
    "stop_words_pre = list(open('data/stop-words.txt', 'r'))\n",
    "\n",
    "#Remove the \"junk\" code for new lines, etc\n",
    "stop_words = [l.strip('\\n\\r') for l in stop_words_pre]\n",
    "\n",
    "#Remove the stop words from Jerry's word frequency list\n",
    "for word in j_ordered_word_freq:\n",
    "    if word in stop_words:\n",
    "        j_ordered_word_freq.remove(word)\n",
    "jerrys_ten = j_ordered_word_freq[:10]\n",
    "\n",
    "for word in e_ordered_word_freq:\n",
    "    if word in stop_words:\n",
    "        e_ordered_word_freq.remove(word)\n",
    "elaines_ten = e_ordered_word_freq[:10]\n",
    "\n",
    "for word in g_ordered_word_freq:\n",
    "    if word in stop_words:\n",
    "        g_ordered_word_freq.remove(word)\n",
    "georges_ten = g_ordered_word_freq[:10]\n",
    "\n",
    "for word in k_ordered_word_freq:\n",
    "    if word in stop_words:\n",
    "        k_ordered_word_freq.remove(word)\n",
    "kramers_ten = k_ordered_word_freq[:10]\n",
    "\n",
    "# Put them all in a dictionary\n",
    "common_words = {'JERRY': jerrys_ten,\n",
    "               'ELAINE': elaines_ten,\n",
    "               'GEORGE': georges_ten,\n",
    "               'KRAMER': kramers_ten}\n",
    "\n",
    "print(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
